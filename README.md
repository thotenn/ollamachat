# Ollama Chat App

Una aplicaciÃ³n mÃ³vil de React Native Expo para chatear con modelos de Ollama en tu servidor local.

## CaracterÃ­sticas

- ðŸ’¬ Interfaz de chat intuitiva
- ðŸ”§ ConfiguraciÃ³n flexible del servidor Ollama
- ðŸ¤– Soporte para mÃºltiples modelos
- ðŸ’¾ Persistencia de configuraciÃ³n
- ðŸ”„ Respuestas en streaming en tiempo real
- ðŸ“± DiseÃ±o responsivo para iOS y Android

## Requisitos previos

- Node.js (v16 o superior)
- npm o yarn
- Expo CLI
- Servidor Ollama ejecutÃ¡ndose (local o remoto)

## InstalaciÃ³n

1. Clona o descarga este proyecto
2. Navega al directorio del proyecto:
   ```bash
   cd ollamachat
   ```
3. Instala las dependencias:
   ```bash
   npm install
   ```

## ConfiguraciÃ³n de Ollama

AsegÃºrate de que tu servidor Ollama estÃ© configurado para aceptar conexiones desde tu dispositivo mÃ³vil:

1. Si estÃ¡s usando Ollama localmente, configÃºralo para escuchar en todas las interfaces:
   ```bash
   OLLAMA_HOST=0.0.0.0:11434 ollama serve
   ```

2. Si estÃ¡s en un dispositivo fÃ­sico, usa la IP de tu computadora (ej: `http://192.168.1.100:11434`)

## Ejecutar la aplicaciÃ³n

### En Expo Go (recomendado para desarrollo)

```bash
npm start
```

Luego escanea el cÃ³digo QR con la app Expo Go en tu dispositivo.

### En iOS Simulator

```bash
npm run ios
```

### En Android Emulator

```bash
npm run android
```

### En Web (desarrollo)

```bash
npm run web
```

## Uso

1. **ConfiguraciÃ³n inicial**: Ve a la pestaÃ±a "ConfiguraciÃ³n"
2. **Configura el servidor**: Ingresa la URL de tu servidor Ollama
3. **Prueba la conexiÃ³n**: Presiona "Probar" para verificar la conexiÃ³n
4. **Selecciona un modelo**: Elige uno de los modelos disponibles
5. **Guarda la configuraciÃ³n**: Presiona "Guardar ConfiguraciÃ³n"
6. **Comienza a chatear**: Ve a la pestaÃ±a "Chat" y empieza a conversar

## Estructura del proyecto

```
ollamachat/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/      # Componentes reutilizables
â”‚   â”œâ”€â”€ contexts/        # Contextos de React
â”‚   â”œâ”€â”€ screens/         # Pantallas principales
â”‚   â”œâ”€â”€ services/        # Servicios y APIs
â”‚   â”œâ”€â”€ types/           # Tipos de TypeScript
â”‚   â”œâ”€â”€ utils/           # Utilidades
â”‚   â””â”€â”€ hooks/           # Custom hooks
â”œâ”€â”€ App.tsx              # Componente principal
â”œâ”€â”€ package.json         # Dependencias
â””â”€â”€ tsconfig.json        # ConfiguraciÃ³n TypeScript
```

## SoluciÃ³n de problemas

### No se puede conectar al servidor Ollama

- Verifica que Ollama estÃ© ejecutÃ¡ndose
- AsegÃºrate de usar la IP correcta (no localhost si estÃ¡s en un dispositivo fÃ­sico)
- Verifica que no haya firewalls bloqueando la conexiÃ³n
- En macOS/Linux, puedes necesitar configurar OLLAMA_HOST

### Los modelos no aparecen

- AsegÃºrate de haber descargado al menos un modelo en Ollama:
  ```bash
  ollama pull llama2
  ```

## PersonalizaciÃ³n

Puedes personalizar los estilos editando los archivos en `src/screens/`. Los colores principales estÃ¡n definidos en los StyleSheets de cada componente.

## Licencia

Este proyecto estÃ¡ bajo la licencia MIT.